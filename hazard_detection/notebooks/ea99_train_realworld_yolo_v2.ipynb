{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-07 20:12:39,536 - INFO - Random seeds set for reproducibility.\n",
      "2025-04-07 20:12:39,555 - INFO - Found 0 images without labels.\n",
      "2025-04-07 20:12:39,557 - INFO - Collected 1140 images. Train: 912, Val: 228\n",
      "Copying to images: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 912/912 [00:00<00:00, 2167.96it/s]\n",
      "Copying to images: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 228/228 [00:00<00:00, 2252.13it/s]\n",
      "2025-04-07 20:12:40,162 - INFO - Train class counts: {1: 166, 2: 79, 3: 12, 0: 78, 4: 81}\n",
      "2025-04-07 20:12:40,163 - INFO - Val class counts: {4: 20, 1: 36, 2: 22, 0: 14, 3: 6}\n",
      "2025-04-07 20:12:40,318 - INFO - Saved class distribution plot.\n",
      "2025-04-07 20:12:42,657 - INFO - Created data.yaml at /home/r0jin/projects/EnigmaAI/yolo_cvat/data.yaml\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New https://pypi.org/project/ultralytics/8.3.103 available ðŸ˜ƒ Update with 'pip install -U ultralytics'\n",
      "Ultralytics YOLOv8.0.239 ðŸš€ Python-3.10.12 torch-2.5.1+cu121 CUDA:0 (NVIDIA GeForce RTX 2070 with Max-Q Design, 7959MiB)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=yolov8s.pt, data=/home/r0jin/projects/EnigmaAI/yolo_cvat/data.yaml, epochs=50, time=None, patience=10, batch=8, imgsz=640, save=True, save_period=-1, cache=False, device=cuda, workers=8, project=runs/train, name=cvat_finetune_all_classes, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=False, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=True, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.001, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=15, translate=0.2, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.2, copy_paste=0.0, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs/train/cvat_finetune_all_classes\n",
      "Overriding model.yaml nc=80 with nc=5\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       928  ultralytics.nn.modules.conv.Conv             [3, 32, 3, 2]                 \n",
      "  1                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  2                  -1  1     29056  ultralytics.nn.modules.block.C2f             [64, 64, 1, True]             \n",
      "  3                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  4                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  5                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  6                  -1  2    788480  ultralytics.nn.modules.block.C2f             [256, 256, 2, True]           \n",
      "  7                  -1  1   1180672  ultralytics.nn.modules.conv.Conv             [256, 512, 3, 2]              \n",
      "  8                  -1  1   1838080  ultralytics.nn.modules.block.C2f             [512, 512, 1, True]           \n",
      "  9                  -1  1    656896  ultralytics.nn.modules.block.SPPF            [512, 512, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    591360  ultralytics.nn.modules.block.C2f             [768, 256, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 16                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 19                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1   1969152  ultralytics.nn.modules.block.C2f             [768, 512, 1]                 \n",
      " 22        [15, 18, 21]  1   2117983  ultralytics.nn.modules.head.Detect           [5, [128, 256, 512]]          \n",
      "Model summary: 225 layers, 11137535 parameters, 11137519 gradients, 28.7 GFLOPs\n",
      "\n",
      "Transferred 349/355 items from pretrained weights\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/train/cvat_finetune_all_classes', view at http://localhost:6006/\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks with YOLOv8n...\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed âœ…\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /home/r0jin/projects/EnigmaAI/yolo_cvat/labels/train... 912 images, 642 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 912/912 [00:00<00:00, 2760.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /home/r0jin/projects/EnigmaAI/yolo_cvat/labels/train.cache\n",
      "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, num_output_channels=3, method='weighted_average'), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/home/r0jin/projects/EnigmaAI/enigmaAI_env/lib/python3.10/site-packages/ultralytics/data/augment.py:847: UserWarning: Argument(s) 'quality_lower' are not valid for transform ImageCompression\n",
      "  A.ImageCompression(quality_lower=75, p=0.0),\n",
      "/home/r0jin/projects/EnigmaAI/enigmaAI_env/lib/python3.10/site-packages/albumentations/core/composition.py:250: UserWarning: Got processor for bboxes, but no transform to process it.\n",
      "  self._set_keys()\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/r0jin/projects/EnigmaAI/yolo_cvat/labels/val... 228 images, 157 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 228/228 [00:00<00:00, 1122.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /home/r0jin/projects/EnigmaAI/yolo_cvat/labels/val.cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.001' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.001111, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
      "50 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       1/50      2.78G      2.067      7.685      1.984          6        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 114/114 [00:18<00:00,  6.24it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:01<00:00,  7.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        228         98      0.601      0.133      0.135     0.0743\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       2/50      2.24G      2.188      4.121      2.206         14        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 114/114 [00:16<00:00,  7.08it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:01<00:00,  8.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        228         98      0.879     0.0333     0.0386     0.0173\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       3/50       2.3G      2.365      4.479      2.381         10        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 114/114 [00:15<00:00,  7.26it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:01<00:00,  9.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        228         98      0.668     0.0167      0.013    0.00472\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       4/50      2.23G        2.5      4.194      2.513          5        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 114/114 [00:15<00:00,  7.14it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:01<00:00,  8.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        228         98      0.607     0.0455     0.0144    0.00527\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       5/50       2.3G      2.547      4.416      2.552          7        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 114/114 [00:15<00:00,  7.18it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:01<00:00,  8.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        228         98    0.00414       0.28     0.0239    0.00664\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       6/50      2.21G      2.594      4.413      2.614          8        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 114/114 [00:16<00:00,  7.08it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:01<00:00,  8.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        228         98      0.402     0.0727     0.0222    0.00849\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       7/50       2.3G      2.546      4.268      2.566          3        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 114/114 [00:17<00:00,  6.67it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:01<00:00,  8.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        228         98      0.828     0.0278     0.0197    0.00606\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       8/50       2.3G       2.37      4.074      2.426          5        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 114/114 [00:16<00:00,  7.00it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:01<00:00,  8.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        228         98       0.71     0.0601     0.0694     0.0299\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       9/50      2.28G      2.273      3.966      2.357          5        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 114/114 [00:16<00:00,  7.10it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:01<00:00,  9.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        228         98      0.782     0.0621      0.133      0.058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      10/50      2.23G      2.246      3.878      2.349         14        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 114/114 [00:16<00:00,  6.95it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:01<00:00,  8.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        228         98     0.0738      0.143     0.0712     0.0252\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      11/50      2.28G      2.164      3.677      2.323         12        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 114/114 [00:16<00:00,  6.93it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:01<00:00,  8.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        228         98        0.9     0.0636      0.099     0.0374\n",
      "Stopping training early as no improvement observed in last 10 epochs. Best results observed at epoch 1, best model saved as best.pt.\n",
      "To update EarlyStopping(patience=10) pass a new patience value, i.e. `patience=300` or use `patience=0` to disable EarlyStopping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "11 epochs completed in 0.069 hours.\n",
      "Optimizer stripped from runs/train/cvat_finetune_all_classes/weights/last.pt, 22.5MB\n",
      "Optimizer stripped from runs/train/cvat_finetune_all_classes/weights/best.pt, 22.5MB\n",
      "\n",
      "Validating runs/train/cvat_finetune_all_classes/weights/best.pt...\n",
      "Ultralytics YOLOv8.0.239 ðŸš€ Python-3.10.12 torch-2.5.1+cu121 CUDA:0 (NVIDIA GeForce RTX 2070 with Max-Q Design, 7959MiB)\n",
      "Model summary (fused): 168 layers, 11127519 parameters, 0 gradients, 28.4 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:03<00:00,  4.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        228         98      0.597      0.229      0.211     0.0895\n",
      "                  hole        228         14          1          0    0.00385    0.00087\n",
      "                  pole        228         36       0.29      0.278       0.18     0.0846\n",
      "                stairs        228         22          1          0      0.022     0.0106\n",
      "          bottle/glass        228          6      0.615      0.667      0.771       0.29\n",
      "                  rock        228         20     0.0781        0.2     0.0771     0.0614\n",
      "Speed: 0.3ms preprocess, 12.3ms inference, 0.0ms loss, 0.9ms postprocess per image\n",
      "mAP50: 0.2108263839100303\n",
      "mAP50-95: 0.0894598172101645\n",
      "Precision: [          1     0.29005           1     0.61482    0.078085]\n",
      "Recall: [          0     0.27778           0     0.66667         0.2]\n",
      "hole - mAP50: 0.0008704460162703442\n",
      "pole - mAP50: 0.08461082574586559\n",
      "stairs - mAP50: 0.010584799581273226\n",
      "bottle/glass - mAP50: 0.2898644177574071\n",
      "rock - mAP50: 0.06136859695000617\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import random\n",
    "import numpy as np\n",
    "import cv2\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from ultralytics import YOLO\n",
    "import logging\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# --- Setup ---\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(42)\n",
    "logger.info(\"Random seeds set for reproducibility.\")\n",
    "\n",
    "# --- Paths ---\n",
    "base_dir = Path('../datasets').resolve()\n",
    "cvat_dir = base_dir / 'cvat_upload'\n",
    "cvat_images_dir = cvat_dir / 'images'\n",
    "cvat_labels_dir = cvat_dir / 'labels'\n",
    "\n",
    "yolo_dir = Path('../yolo_cvat').resolve()\n",
    "train_img_dir = yolo_dir / 'images' / 'train'\n",
    "val_img_dir = yolo_dir / 'images' / 'val'\n",
    "train_lbl_dir = yolo_dir / 'labels' / 'train'\n",
    "val_lbl_dir = yolo_dir / 'labels' / 'val'\n",
    "\n",
    "for d in [train_img_dir, val_img_dir, train_lbl_dir, val_lbl_dir]:\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# --- Collect and Split Data ---\n",
    "images = list(cvat_images_dir.glob('*.jpg'))\n",
    "labels = list(cvat_labels_dir.glob('*.txt'))\n",
    "\n",
    "# Identify images without labels\n",
    "images_without_labels = [img for img in images if not (cvat_labels_dir / f\"{img.stem}.txt\").exists()]\n",
    "logger.info(f\"Found {len(images_without_labels)} images without labels.\")\n",
    "\n",
    "# Create empty label files for missing\n",
    "for img in images_without_labels:\n",
    "    empty_lbl = cvat_labels_dir / f\"{img.stem}.txt\"\n",
    "    open(empty_lbl, 'w').close()\n",
    "    logger.info(f\"Created empty label file: {empty_lbl}\")\n",
    "\n",
    "train_imgs, val_imgs = train_test_split(images, test_size=0.2, random_state=42)\n",
    "logger.info(f\"Collected {len(images)} images. Train: {len(train_imgs)}, Val: {len(val_imgs)}\")\n",
    "\n",
    "# Copy images and labels\n",
    "for split_imgs, img_dst, lbl_dst in [(train_imgs, train_img_dir, train_lbl_dir), \n",
    "                                     (val_imgs, val_img_dir, val_lbl_dir)]:\n",
    "    for img in tqdm(split_imgs, desc=f\"Copying to {img_dst.parent.name}\"):\n",
    "        lbl = cvat_labels_dir / f\"{img.stem}.txt\"\n",
    "        shutil.copy(img, img_dst / img.name)\n",
    "        if lbl.exists():\n",
    "            shutil.copy(lbl, lbl_dst / lbl.name)\n",
    "        else:\n",
    "            open(lbl_dst / f\"{img.stem}.txt\", 'w').close()\n",
    "\n",
    "# --- Define classes ---\n",
    "selected_classes = ['hole', 'pole', 'stairs', 'bottle/glass', 'rock']\n",
    "\n",
    "# --- Check Annotations ---\n",
    "def check_annotations(label_dir, num_classes):\n",
    "    for lbl_file in label_dir.glob('*.txt'):\n",
    "        with open(lbl_file, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "        for line in lines:\n",
    "            if line.strip():\n",
    "                parts = line.split()\n",
    "                if len(parts) != 5:\n",
    "                    logger.error(f\"Invalid format in {lbl_file}: {line}\")\n",
    "                    continue\n",
    "                cls_id = int(parts[0])\n",
    "                if cls_id >= num_classes or cls_id < 0:\n",
    "                    logger.error(f\"Invalid class ID {cls_id} in {lbl_file}\")\n",
    "                for val in parts[1:]:\n",
    "                    if not 0 <= float(val) <= 1:\n",
    "                        logger.error(f\"Out-of-range value in {lbl_file}: {val}\")\n",
    "\n",
    "check_annotations(train_lbl_dir, len(selected_classes))\n",
    "check_annotations(val_lbl_dir, len(selected_classes))\n",
    "\n",
    "# --- Class Distribution ---\n",
    "def get_class_counts(label_dir):\n",
    "    class_counts = Counter()\n",
    "    for lbl_file in label_dir.glob('*.txt'):\n",
    "        with open(lbl_file, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "        for line in lines:\n",
    "            if line.strip():\n",
    "                cls_id = int(line.split()[0])\n",
    "                class_counts[cls_id] += 1\n",
    "    return class_counts\n",
    "\n",
    "train_class_counts = get_class_counts(train_lbl_dir)\n",
    "val_class_counts = get_class_counts(val_lbl_dir)\n",
    "logger.info(f\"Train class counts: {dict(train_class_counts)}\")\n",
    "logger.info(f\"Val class counts: {dict(val_class_counts)}\")\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.bar(selected_classes, [train_class_counts.get(i, 0) for i in range(len(selected_classes))], \n",
    "        alpha=0.5, label='Train')\n",
    "plt.bar(selected_classes, [val_class_counts.get(i, 0) for i in range(len(selected_classes))], \n",
    "        alpha=0.5, label='Val')\n",
    "plt.xlabel('Classes')\n",
    "plt.ylabel('Number of Instances')\n",
    "plt.title('Class Distribution')\n",
    "plt.legend()\n",
    "plt.savefig('class_distribution.png')\n",
    "plt.close()\n",
    "logger.info(\"Saved class distribution plot.\")\n",
    "\n",
    "# --- Visualize Annotations ---\n",
    "def visualize_annotations(image_path, label_path, class_names):\n",
    "    img = cv2.imread(str(image_path))\n",
    "    if img is None:\n",
    "        logger.error(f\"Failed to load image: {image_path}\")\n",
    "        return None\n",
    "    h, w = img.shape[:2]\n",
    "    with open(label_path, 'r') as f:\n",
    "        for line in f:\n",
    "            if line.strip():\n",
    "                cls_id, x, y, width, height = map(float, line.split())\n",
    "                x1 = int((x - width/2) * w)\n",
    "                y1 = int((y - height/2) * h)\n",
    "                x2 = int((x + width/2) * w)\n",
    "                y2 = int((y + height/2) * h)\n",
    "                cv2.rectangle(img, (x1, y1), (x2, y2), (0,255,0), 2)\n",
    "                label = class_names[int(cls_id)]\n",
    "                cv2.putText(img, label, (x1, y1 - 10),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0,255,0), 2)\n",
    "    return cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "def plot_samples_per_class(image_dir, label_dir, class_names, num_samples=3):\n",
    "    class_images = {i: [] for i in range(len(class_names))}\n",
    "    for lbl_file in label_dir.glob('*.txt'):\n",
    "        with open(lbl_file, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "        classes_in_image = set()\n",
    "        for line in lines:\n",
    "            if line.strip():\n",
    "                cls_id = int(line.split()[0])\n",
    "                classes_in_image.add(cls_id)\n",
    "        for c_id in classes_in_image:\n",
    "            class_images[c_id].append(lbl_file.stem)\n",
    "    \n",
    "    for cls_id, stems in class_images.items():\n",
    "        if not stems:\n",
    "            logger.warning(f\"No images found for class {class_names[cls_id]}\")\n",
    "            continue\n",
    "        selected_stems = random.sample(stems, min(num_samples, len(stems)))\n",
    "        for s in selected_stems:\n",
    "            img_path = image_dir / f\"{s}.jpg\"\n",
    "            lbl_path = label_dir / f\"{s}.txt\"\n",
    "            annotated = visualize_annotations(img_path, lbl_path, class_names)\n",
    "            if annotated is not None:\n",
    "                plt.figure(figsize=(8,6))\n",
    "                plt.imshow(annotated)\n",
    "                plt.title(f\"{class_names[cls_id]} - {s}\")\n",
    "                plt.axis('off')\n",
    "                safe_class_name = class_names[cls_id].replace(\"/\", \"_\")  # <-- fix here\n",
    "                plt.savefig(f\"annotation_{safe_class_name}_{s}.png\")\n",
    "                plt.close()\n",
    "\n",
    "\n",
    "plot_samples_per_class(train_img_dir, train_lbl_dir, selected_classes, 3)\n",
    "\n",
    "# --- YOLO data.yaml ---\n",
    "data_yaml_path = yolo_dir / 'data.yaml'\n",
    "with open(data_yaml_path, 'w') as f:\n",
    "    f.write(f\"train: {train_img_dir.resolve()}\\n\")\n",
    "    f.write(f\"val: {val_img_dir.resolve()}\\n\")\n",
    "    f.write(f\"nc: {len(selected_classes)}\\n\")\n",
    "    f.write(f\"names: {selected_classes}\\n\")\n",
    "logger.info(f\"Created data.yaml at {data_yaml_path}\")\n",
    "\n",
    "# --- Train YOLO (Single Model) ---\n",
    "model = YOLO('yolov8s.pt')  \n",
    "\n",
    "results = model.train(\n",
    "    data=str(data_yaml_path),\n",
    "    epochs=50,\n",
    "    imgsz=640,\n",
    "    batch=8,\n",
    "    lr0=0.001,\n",
    "    warmup_epochs=3,\n",
    "    device='cuda' if torch.cuda.is_available() else 'cpu',\n",
    "    name=\"cvat_finetune_all_classes\",\n",
    "    project=\"runs/train\",\n",
    "    patience=10,\n",
    "    verbose=True,\n",
    "    plots=False,\n",
    "    augment=True,\n",
    "    mosaic=1.0,\n",
    "    mixup=0.2,\n",
    "    degrees=15,\n",
    "    translate=0.2\n",
    ")\n",
    "\n",
    "# --- Print Metrics ---\n",
    "print(f\"mAP50: {results.box.map50}\")\n",
    "print(f\"mAP50-95: {results.box.map}\")\n",
    "print(f\"Precision: {results.box.p}\")\n",
    "print(f\"Recall: {results.box.r}\")\n",
    "for i, cls in enumerate(selected_classes):\n",
    "    print(f\"{cls} - mAP50: {results.box.maps[i]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 /home/r0jin/projects/EnigmaAI/code/../datasets/cvat_upload/images/frame_IMG_4322_00024.jpg: 640x640 1 pole, 8.0ms\n",
      "Speed: 1.3ms preprocess, 8.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "image 1/1 /home/r0jin/projects/EnigmaAI/code/../datasets/cvat_upload/images/frame_IMG_4322_00024.jpg: 640x640 (no detections), 8.3ms\n",
      "Speed: 1.6ms preprocess, 8.3ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# --- Paths to trained weights ---\n",
    "POLE_MODEL_PATH = \"runs/train/subset_test22/weights/best.pt\"  # Older 'pole' model\n",
    "NEW_MODEL_PATH = \"runs/train/cvat_finetune_no_pole/weights/best.pt\"  \n",
    "\n",
    "# Load each model\n",
    "pole_model = YOLO(POLE_MODEL_PATH)\n",
    "other_model = YOLO(NEW_MODEL_PATH)\n",
    "\n",
    "\n",
    "# ['door','table','openedDoor','chair','pole','bike','truck','car','dog','bus']\n",
    "# Then 'pole' is index 4. \n",
    "\n",
    "pole_class_id_in_old_model = 4\n",
    "other_classes = ['hole','stairs','bottle/glass','rock']  \n",
    "\n",
    "def combine_detections(image_path, pole_conf=0.25, other_conf=0.25):\n",
    "    \"\"\"\n",
    "    Runs inference from two YOLO models:\n",
    "    1) pole_model  - we select only 'pole' from it\n",
    "    2) other_model - for the other classes\n",
    "\n",
    "    Returns an annotated image with bounding boxes from both.\n",
    "    \"\"\"\n",
    "    img = cv2.imread(str(image_path))\n",
    "    if img is None:\n",
    "        print(f\"Failed to load image: {image_path}\")\n",
    "        return None\n",
    "    \n",
    "    # 1) Inference for 'pole' only\n",
    "    pole_results = pole_model.predict(source=image_path, conf=pole_conf, classes=[pole_class_id_in_old_model])\n",
    "    #   classes=[pole_class_id_in_old_model] ensures we only keep detection with old model's class=4 ('pole')\n",
    "    pole_boxes = pole_results[0].boxes\n",
    "\n",
    "    # 2) Inference for other classes\n",
    " \n",
    "    other_results = other_model.predict(source=image_path, conf=other_conf)\n",
    "    other_boxes = other_results[0].boxes\n",
    "    \n",
    "    # Convert boxes to a combined list\n",
    "    detections = []\n",
    "\n",
    "    # Add 'pole' from old model\n",
    "    for box in pole_boxes:\n",
    "        xyxy = box.xyxy[0].cpu().numpy()\n",
    "        conf = float(box.conf.cpu().numpy())\n",
    "        # old model's class name is 'pole' only for those we forced, so:\n",
    "        detections.append({\n",
    "            'xyxy': xyxy,\n",
    "            'conf': conf,\n",
    "            'class_name': 'pole'\n",
    "        })\n",
    "\n",
    "    # Add other classes from new model\n",
    "    for box in other_boxes:\n",
    "        xyxy = box.xyxy[0].cpu().numpy()\n",
    "        conf = float(box.conf.cpu().numpy())\n",
    "        cls_id = int(box.cls.cpu().numpy())\n",
    "        # Map the new model's class ID to a string label\n",
    "        class_name = other_classes[cls_id] if cls_id < len(other_classes) else str(cls_id)\n",
    "        detections.append({\n",
    "            'xyxy': xyxy,\n",
    "            'conf': conf,\n",
    "            'class_name': class_name\n",
    "        })\n",
    "    \n",
    "    # Visualize\n",
    "    for det in detections:\n",
    "        x1, y1, x2, y2 = map(int, det['xyxy'])\n",
    "        cv2.rectangle(img, (x1, y1), (x2, y2), (0,255,0), 2)\n",
    "        label = f\"{det['class_name']} {det['conf']:.2f}\"\n",
    "        cv2.putText(img, label, (x1, y1 - 10),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0,255,0), 2)\n",
    "    \n",
    "    return cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# Example usage:\n",
    "image_path = \"../datasets/cvat_upload/images/frame_IMG_4322_00024.jpg\"\n",
    "annotated_img = combine_detections(image_path)\n",
    "if annotated_img is not None:\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.imshow(annotated_img)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "enigmaAI_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
